"""
Deep Workflow Environment - çœŸæ­£çš„AFlow Workflowæ‰§è¡Œç¯å¢ƒ
Real AFlow workflow execution environment with actual code testing
"""

import sys
import os
import asyncio
import shutil
import importlib
from typing import List, Tuple, Dict, Optional
import numpy as np

# Add paths
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'AFlow'))
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'AFlow', 'scripts'))

from scripts.logs import logger
from scripts.evaluator import DatasetType
from workflow_parser import WorkflowParser, WorkflowSpec
from workflow_evaluator import WorkflowEvaluator


class DeepWorkflowEnv:
    """
    æ·±åº¦é›†æˆçš„Workflowç¯å¢ƒ

    åŠŸèƒ½ï¼š
    1. æ¥æ”¶Qwenç”Ÿæˆçš„workflowæè¿°
    2. è§£æå¹¶ç”Ÿæˆå¯æ‰§è¡Œçš„workflowä»£ç 
    3. æ‰§è¡ŒçœŸå®çš„HumanEvalæµ‹è¯•
    4. è¿”å›çœŸå®çš„pass@kåˆ†æ•°ä½œä¸ºreward
    """

    def __init__(
        self,
        dataset: str,
        opt_llm_config: Dict,
        exec_llm_config: Dict,
        operators: List[str],
        env_num: int = 2,
        sample: int = 3,
        max_rounds: int = 10,
        workspace_path: str = None
    ):
        """
        åˆå§‹åŒ–çœŸå®workflowç¯å¢ƒ

        Args:
            dataset: æ•°æ®é›†åç§°ï¼ˆå¦‚"HumanEval"ï¼‰
            opt_llm_config: ä¼˜åŒ–LLMé…ç½®ï¼ˆGPT-4oï¼Œç”¨äºworkflowç”Ÿæˆï¼‰
            exec_llm_config: æ‰§è¡ŒLLMé…ç½®ï¼ˆç”¨äºè¿è¡Œworkflowä¸­çš„LLMè°ƒç”¨ï¼‰
            operators: å¯ç”¨çš„operatorsåˆ—è¡¨
            env_num: å¹¶è¡Œç¯å¢ƒæ•°é‡
            sample: æ¯è½®æµ‹è¯•çš„æ ·æœ¬æ•°
            max_rounds: æœ€å¤§è½®æ•°
            workspace_path: workspaceè·¯å¾„ï¼ˆå­˜å‚¨workflowä»£ç ï¼‰
        """
        self.dataset = dataset
        self.opt_llm_config = opt_llm_config
        self.exec_llm_config = exec_llm_config
        self.operators = operators
        self.env_num = env_num
        self.sample = sample
        self.max_rounds = max_rounds

        # Workspaceè·¯å¾„ï¼ˆå­˜å‚¨ç”Ÿæˆçš„workflowï¼‰
        if workspace_path is None:
            aflow_path = os.path.join(os.path.dirname(__file__), '..', 'AFlow')
            self.workspace_path = os.path.join(aflow_path, 'workspace', dataset, 'workflows_rl')
        else:
            self.workspace_path = workspace_path

        os.makedirs(self.workspace_path, exist_ok=True)

        # åˆ›å»ºworkflowè§£æå™¨
        self.workflow_parser = WorkflowParser()

        # åˆ›å»ºevaluatorï¼ˆç”¨äºçœŸå®æµ‹è¯•ï¼‰
        self.evaluator = WorkflowEvaluator(
            dataset="HumanEval",
            sample_size=sample,
            timeout_per_problem=30
        )

        # å½“å‰çŠ¶æ€
        self.current_round = 0
        self.workflow_history = []  # å†å²workflowåŠå…¶åˆ†æ•°
        self.best_score = 0.0
        self.best_workflow = None

        # ç»Ÿè®¡
        self.total_api_calls = 0
        self.total_tests_run = 0

        logger.info(f"[DeepWorkflowEnv] Initialized")
        logger.info(f"[DeepWorkflowEnv] Dataset: {dataset}")
        logger.info(f"[DeepWorkflowEnv] Workspace: {self.workspace_path}")
        logger.info(f"[DeepWorkflowEnv] Evaluator sample size: {sample}")
        logger.info(f"[DeepWorkflowEnv] âœ… REAL WORKFLOW EXECUTION ENABLED")

    def reset(self) -> Tuple[List[str], List[Dict]]:
        """
        é‡ç½®ç¯å¢ƒ

        Returns:
            observations: è§‚æµ‹åˆ—è¡¨
            info: ä¿¡æ¯å­—å…¸åˆ—è¡¨
        """
        self.current_round = 0

        observations = []
        info = []

        for i in range(self.env_num):
            # æ„é€ è§‚æµ‹ï¼šå‘Šè¯‰Qwenå½“å‰çŠ¶æ€
            obs = self._construct_observation(
                round_num=0,
                best_score=self.best_score,
                history_summary=self._get_history_summary()
            )
            observations.append(obs)

            info_dict = {
                'step': 0,
                'round': 0,
                'env_id': i,
                'best_score': self.best_score,
                'workflow_path': None
            }
            info.append(info_dict)

        logger.info(f"[DeepWorkflowEnv] Environment reset")
        return observations, info

    def step(self, actions: List[str]) -> Tuple[List[str], List[float], List[bool], List[Dict]]:
        """
        æ‰§è¡Œstep - è¿™é‡Œæ˜¯çœŸæ­£çš„workflowæ‰§è¡Œï¼

        Args:
            actions: Qwenç”Ÿæˆçš„workflowæè¿°åˆ—è¡¨

        Returns:
            next_observations: ä¸‹ä¸€æ­¥è§‚æµ‹
            rewards: çœŸå®çš„workflowæ€§èƒ½åˆ†æ•°ï¼ˆpass@kï¼‰
            dones: æ˜¯å¦ç»“æŸ
            info: é¢å¤–ä¿¡æ¯
        """
        self.current_round += 1

        next_observations = []
        rewards = []
        dones = []
        info = []

        logger.info(f"[DeepWorkflowEnv] ===== Round {self.current_round} =====")
        logger.info(f"[DeepWorkflowEnv] Processing {len(actions)} workflow proposals...")

        for i, qwen_action in enumerate(actions):
            try:
                logger.info(f"[DeepWorkflowEnv] Env {i}: Processing Qwen output...")
                logger.info(f"[DeepWorkflowEnv] Env {i}: Action preview: {qwen_action[:200]}...")

                # 1. è§£æQwenè¾“å‡ºä¸ºworkflowè§„æ ¼
                workflow_spec = self.workflow_parser.parse_qwen_output(qwen_action)

                if workflow_spec is None:
                    logger.error(f"[DeepWorkflowEnv] Env {i}: Failed to parse Qwen output!")
                    rewards.append(0.0)
                    next_observations.append(self._construct_observation(
                        self.current_round, self.best_score, "Parse failed"
                    ))
                    dones.append(False)
                    info.append({'step': self.current_round, 'error': 'parse_failed'})
                    continue

                logger.info(f"[DeepWorkflowEnv] Env {i}: Parsed workflow:")
                logger.info(f"[DeepWorkflowEnv] Env {i}:   Modification: {workflow_spec.modification}")
                logger.info(f"[DeepWorkflowEnv] Env {i}:   Operators: {workflow_spec.operators}")

                # 2. ä¿å­˜workflowä»£ç åˆ°æ–‡ä»¶
                round_id = f"{self.current_round}_env{i}"
                workflow_path = self.workflow_parser.save_workflow_to_file(
                    workflow_spec,
                    round_id,
                    self.workspace_path
                )

                logger.info(f"[DeepWorkflowEnv] Env {i}: Workflow code saved to {workflow_path}")

                # 3. æ‰§è¡ŒçœŸå®çš„workflowæµ‹è¯•ï¼
                logger.info(f"[DeepWorkflowEnv] Env {i}: âš¡ EXECUTING REAL WORKFLOW TEST...")
                score = self._execute_workflow_test(round_id, workflow_path)

                self.total_tests_run += 1

                logger.info(f"[DeepWorkflowEnv] Env {i}: âœ… Real test score: {score:.4f}")
                logger.info(f"[DeepWorkflowEnv] Env {i}: This is a REAL pass@k score from HumanEval!")

                # 4. æ›´æ–°æœ€ä½³workflow
                if score > self.best_score:
                    logger.info(f"[DeepWorkflowEnv] Env {i}: ğŸ‰ NEW BEST SCORE! {self.best_score:.4f} -> {score:.4f}")
                    self.best_score = score
                    self.best_workflow = workflow_spec

                # 5. è®°å½•å†å²
                self.workflow_history.append({
                    'round': self.current_round,
                    'env_id': i,
                    'score': score,
                    'workflow_spec': workflow_spec,
                    'workflow_path': workflow_path
                })

                # 6. è¿”å›çœŸå®åˆ†æ•°ä½œä¸ºreward
                reward = float(score)
                rewards.append(reward)

                # 7. æ„é€ ä¸‹ä¸€ä¸ªè§‚æµ‹
                next_obs = self._construct_observation(
                    round_num=self.current_round,
                    best_score=self.best_score,
                    history_summary=self._get_history_summary(),
                    last_score=score
                )
                next_observations.append(next_obs)

                # 8. åˆ¤æ–­æ˜¯å¦ç»“æŸ
                done = self.current_round >= self.max_rounds
                dones.append(done)

                # 9. Info
                info_dict = {
                    'step': self.current_round,
                    'round': self.current_round,
                    'env_id': i,
                    'score': score,
                    'best_score': self.best_score,
                    'workflow_path': workflow_path,
                    'operators': workflow_spec.operators,
                    'modification': workflow_spec.modification,
                    'is_best': score == self.best_score
                }
                info.append(info_dict)

            except Exception as e:
                logger.error(f"[DeepWorkflowEnv] Env {i}: ERROR: {e}")
                import traceback
                traceback.print_exc()

                rewards.append(0.0)
                next_observations.append(self._construct_observation(
                    self.current_round, self.best_score, f"Error: {str(e)}"
                ))
                dones.append(False)
                info.append({'step': self.current_round, 'error': str(e)})

        avg_reward = np.mean(rewards) if rewards else 0.0
        logger.info(f"[DeepWorkflowEnv] Round {self.current_round} completed")
        logger.info(f"[DeepWorkflowEnv] Avg reward: {avg_reward:.4f}, Best so far: {self.best_score:.4f}")
        logger.info(f"[DeepWorkflowEnv] Total tests run: {self.total_tests_run}")

        return next_observations, rewards, dones, info

    def _execute_workflow_test(self, round_id: str, workflow_path: str) -> float:
        """
        æ‰§è¡ŒçœŸå®çš„workflowæµ‹è¯•

        Args:
            round_id: round ID
            workflow_path: workflowä»£ç è·¯å¾„

        Returns:
            çœŸå®çš„pass@kåˆ†æ•°ï¼ˆ0.0-1.0ï¼‰
        """
        try:
            # å¯¼å…¥workflowæ¨¡å—
            round_dir = os.path.dirname(workflow_path)
            module_name = f"workspace.{self.dataset}.workflows_rl.round_{round_id}.graph"

            # åŠ¨æ€å¯¼å…¥
            spec = importlib.util.spec_from_file_location(module_name, workflow_path)
            module = importlib.util.module_from_spec(spec)
            sys.modules[module_name] = module
            spec.loader.exec_module(module)

            # è·å–Workflowç±»
            WorkflowClass = module.Workflow

            # åˆ›å»ºworkflowå®ä¾‹
            workflow = WorkflowClass(
                name=f"RL_Workflow_R{round_id}",
                llm_config=self.exec_llm_config,
                dataset=self.dataset
            )

            # ä½¿ç”¨evaluatoræ‰§è¡Œæµ‹è¯•
            # è¿™ä¼šçœŸæ­£è¿è¡ŒHumanEvalä»»åŠ¡å¹¶è¿”å›pass@k
            logger.info(f"[DeepWorkflowEnv] Running real HumanEval test with sample={self.sample}...")

            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

            # æ‰§è¡Œè¯„ä¼°
            result = loop.run_until_complete(
                self.evaluator.evaluate_workflow(workflow)
            )

            loop.close()

            # resultæ˜¯è¯„ä¼°ç»“æœdictï¼Œæå–pass@kåˆ†æ•°
            score = result['pass_at_k'] if result and 'pass_at_k' in result else 0.0
            return float(score)

        except Exception as e:
            logger.error(f"[DeepWorkflowEnv] Workflow execution error: {e}")
            import traceback
            traceback.print_exc()
            return 0.0

    def _construct_observation(
        self,
        round_num: int,
        best_score: float,
        history_summary: str,
        last_score: Optional[float] = None
    ) -> str:
        """æ„é€ ç»™Qwençš„è§‚æµ‹"""
        obs = f"""Dataset: {self.dataset}
Task: Design and optimize agent workflow for code generation
Round: {round_num}/{self.max_rounds}

Current Best Score: {best_score:.4f}"""

        if last_score is not None:
            obs += f"\nLast Score: {last_score:.4f}"

        obs += f"""

Available Operators:
{', '.join(self.operators)}

{history_summary}

Your task: Generate a workflow description that will be converted to executable code and tested on real {self.dataset} problems. Focus on:
1. Which operators to use
2. How to combine them effectively
3. How to improve upon previous attempts
"""

        return obs

    def _get_history_summary(self) -> str:
        """è·å–å†å²workflowæ‘˜è¦"""
        if not self.workflow_history:
            return "History: No previous workflows yet."

        # å–æœ€è¿‘3ä¸ªworkflow
        recent = self.workflow_history[-3:]
        summary = "Recent Workflow Performance:\n"

        for item in recent:
            summary += f"  Round {item['round']} Env{item['env_id']}: "
            summary += f"Score={item['score']:.4f}, "
            summary += f"Operators={item['workflow_spec'].operators}\n"

        return summary

    def close(self):
        """å…³é—­ç¯å¢ƒ"""
        logger.info(f"[DeepWorkflowEnv] Environment closed")
        logger.info(f"[DeepWorkflowEnv] Total tests run: {self.total_tests_run}")
        logger.info(f"[DeepWorkflowEnv] Best score achieved: {self.best_score:.4f}")


def create_deep_workflow_env(dataset, opt_llm_config, exec_llm_config, **kwargs):
    """
    åˆ›å»ºæ·±åº¦workflowç¯å¢ƒçš„å·¥å‚å‡½æ•°

    è¿™æ˜¯çœŸæ­£çš„AFlowé›†æˆï¼
    """
    return DeepWorkflowEnv(
        dataset=dataset,
        opt_llm_config=opt_llm_config,
        exec_llm_config=exec_llm_config,
        operators=kwargs.get('operators', ['Custom', 'CustomCodeGenerate', 'ScEnsemble', 'Test']),
        env_num=kwargs.get('env_num', 2),
        sample=kwargs.get('sample', 3),
        max_rounds=kwargs.get('max_rounds', 10),
        workspace_path=kwargs.get('workspace_path')
    )


if __name__ == "__main__":
    # æµ‹è¯•ç¯å¢ƒ
    import yaml

    # åŠ è½½é…ç½®
    config_path = "deep_config_e2e.yaml"
    with open(config_path) as f:
        config = yaml.safe_load(f)

    env_config = config['environment']

    # åˆ›å»ºç¯å¢ƒ
    env = create_deep_workflow_env(
        dataset="HumanEval",
        opt_llm_config=env_config['opt_llm_config'],
        exec_llm_config=env_config['exec_llm_config'],
        operators=env_config['operators'],
        env_num=1,
        sample=2
    )

    # æµ‹è¯•
    obs, info = env.reset()
    print(f"Initial observation:\n{obs[0]}\n")

    # æ¨¡æ‹ŸQwenè¾“å‡º
    test_action = """
<workflow_modification>
Use ensemble approach to improve code quality
</workflow_modification>

<operators>
CustomCodeGenerate, ScEnsemble
</operators>

<workflow_steps>
1. Generate 3 candidate code solutions
2. Use ScEnsemble to select the best one
</workflow_steps>
"""

    next_obs, rewards, dones, info = env.step([test_action])
    print(f"Reward (real pass@k): {rewards[0]:.4f}")
    print(f"This is a REAL score from executing workflow on HumanEval!")
