"""
Workflow Evaluator - çœŸå®æ‰§è¡Œworkflowå¹¶æµ‹è¯•HumanEval
Real workflow execution and HumanEval testing
"""

import sys
import os
import asyncio
import time
from typing import Dict, Any, Optional
import json

# Add AFlow to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'AFlow'))
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'AFlow', 'scripts'))

from scripts.logs import logger
from scripts.evaluator import DatasetType


class WorkflowEvaluator:
    """
    Workflowè¯„ä¼°å™¨ - æ‰§è¡ŒçœŸå®çš„HumanEvalæµ‹è¯•

    åŠŸèƒ½ï¼š
    1. åŠ è½½workflowå®ä¾‹
    2. åœ¨HumanEvalæ•°æ®é›†ä¸Šè¿è¡Œworkflow
    3. è®¡ç®—pass@kåˆ†æ•°
    4. è¿”å›çœŸå®çš„æ€§èƒ½æŒ‡æ ‡
    """

    def __init__(
        self,
        dataset: str = "HumanEval",
        sample_size: int = 3,
        timeout_per_problem: int = 30
    ):
        """
        åˆå§‹åŒ–evaluator

        Args:
            dataset: æ•°æ®é›†åç§°
            sample_size: æµ‹è¯•æ ·æœ¬æ•°é‡
            timeout_per_problem: æ¯ä¸ªé—®é¢˜çš„è¶…æ—¶æ—¶é—´(ç§’)
        """
        self.dataset = dataset
        self.sample_size = sample_size
        self.timeout_per_problem = timeout_per_problem

        # åŠ è½½HumanEvalæ•°æ®é›†
        self.problems = self._load_humaneval_problems()

        logger.info(f"[WorkflowEvaluator] Initialized")
        logger.info(f"[WorkflowEvaluator] Dataset: {dataset}")
        logger.info(f"[WorkflowEvaluator] Sample size: {sample_size}")
        logger.info(f"[WorkflowEvaluator] Loaded {len(self.problems)} problems")

    def _load_humaneval_problems(self) -> Dict:
        """åŠ è½½HumanEvalé—®é¢˜"""
        try:
            # HumanEvalæ•°æ®æ–‡ä»¶è·¯å¾„
            aflow_path = os.path.join(os.path.dirname(__file__), '..', 'AFlow')
            humaneval_path = os.path.join(aflow_path, 'datasets', 'HumanEval.jsonl')

            if not os.path.exists(humaneval_path):
                # å°è¯•å¤‡ç”¨è·¯å¾„
                humaneval_path = os.path.join(aflow_path, 'data', 'HumanEval.jsonl')

            if not os.path.exists(humaneval_path):
                logger.warning(f"[WorkflowEvaluator] HumanEval file not found, using dummy data")
                return self._create_dummy_problems()

            # åŠ è½½problems
            problems = {}
            with open(humaneval_path, 'r') as f:
                for line in f:
                    problem = json.loads(line)
                    problems[problem['task_id']] = problem

            return problems

        except Exception as e:
            logger.error(f"[WorkflowEvaluator] Error loading HumanEval: {e}")
            return self._create_dummy_problems()

    def _create_dummy_problems(self) -> Dict:
        """åˆ›å»ºä¸€äº›dummy problemsç”¨äºæµ‹è¯•"""
        return {
            'HumanEval/0': {
                'task_id': 'HumanEval/0',
                'prompt': 'def has_close_elements(numbers: List[float], threshold: float) -> bool:\n    """ Check if in given list of numbers, are any two numbers closer to each other than\n    given threshold.\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n    False\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n    True\n    """\n',
                'entry_point': 'has_close_elements',
                'canonical_solution': '    for idx, elem in enumerate(numbers):\n        for idx2, elem2 in enumerate(numbers):\n            if idx != idx2:\n                distance = abs(elem - elem2)\n                if distance < threshold:\n                    return True\n\n    return False\n',
                'test': 'def check(candidate):\n    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True\n    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False\n    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) == True\n    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.8) == False\n    assert candidate([1.0, 2.0, 3.0, 4.0, 5.0, 2.0], 0.1) == True\n\ncheck(has_close_elements)'
            }
        }

    async def evaluate_workflow(
        self,
        workflow,
        num_problems: Optional[int] = None,
        use_test_set: bool = False,
        random_sample: bool = True
    ) -> Dict[str, Any]:
        """
        è¯„ä¼°workflowæ€§èƒ½

        Args:
            workflow: Workflowå®ä¾‹
            num_problems: æµ‹è¯•çš„é—®é¢˜æ•°é‡ï¼ˆNone=ä½¿ç”¨sample_sizeï¼‰
            use_test_set: æ˜¯å¦ä½¿ç”¨æµ‹è¯•é›†ï¼ˆTrue=æµ‹è¯•é›†ï¼ŒFalse=è®­ç»ƒé›†ï¼‰
            random_sample: æ˜¯å¦éšæœºé‡‡æ ·ï¼ˆTrue=éšæœºï¼ŒFalse=å›ºå®šå‰Nä¸ªï¼‰

        Returns:
            è¯„ä¼°ç»“æœå­—å…¸ï¼ŒåŒ…å«ï¼š
            - pass_at_k: pass@kåˆ†æ•°
            - num_passed: é€šè¿‡çš„é—®é¢˜æ•°
            - num_total: æ€»é—®é¢˜æ•°
            - avg_time: å¹³å‡æ—¶é—´
            - details: è¯¦ç»†ç»“æœ
        """
        start_time = time.time()

        # é€‰æ‹©æµ‹è¯•é—®é¢˜
        if num_problems is None:
            num_problems = min(self.sample_size, len(self.problems))

        # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›† (80/20 split)
        all_problem_ids = list(self.problems.keys())
        train_size = int(len(all_problem_ids) * 0.8)
        train_ids = all_problem_ids[:train_size]
        test_ids = all_problem_ids[train_size:]

        # é€‰æ‹©æ•°æ®é›†
        if use_test_set:
            available_ids = test_ids
            logger.info(f"[WorkflowEvaluator] ğŸ“Š Using TEST set ({len(test_ids)} problems available)")
        else:
            available_ids = train_ids
            logger.info(f"[WorkflowEvaluator] ğŸ“š Using TRAIN set ({len(train_ids)} problems available)")

        # é‡‡æ ·é—®é¢˜
        if random_sample and num_problems < len(available_ids):
            import random
            problem_ids = random.sample(available_ids, min(num_problems, len(available_ids)))
            logger.info(f"[WorkflowEvaluator] ğŸ² Randomly sampled {len(problem_ids)} problems")
        else:
            problem_ids = available_ids[:min(num_problems, len(available_ids))]
            logger.info(f"[WorkflowEvaluator] ğŸ“‹ Using first {len(problem_ids)} problems")

        logger.info(f"[WorkflowEvaluator] Testing workflow on {len(problem_ids)} problems...")

        results = []
        num_passed = 0

        for i, task_id in enumerate(problem_ids):
            problem = self.problems[task_id]

            logger.info(f"[WorkflowEvaluator] [{i+1}/{num_problems}] Testing {task_id}...")

            try:
                # è¿è¡Œworkflow
                problem_start = time.time()

                solution, cost = await asyncio.wait_for(
                    workflow(
                        problem=problem['prompt'],
                        entry_point=problem['entry_point']
                    ),
                    timeout=self.timeout_per_problem
                )

                problem_time = time.time() - problem_start

                # æµ‹è¯•solution
                passed = self._test_solution(
                    solution=solution,
                    test_code=problem.get('test', ''),
                    entry_point=problem['entry_point']
                )

                if passed:
                    num_passed += 1
                    logger.info(f"[WorkflowEvaluator] {task_id}: âœ… PASSED")
                else:
                    logger.info(f"[WorkflowEvaluator] {task_id}: âŒ FAILED")

                results.append({
                    'task_id': task_id,
                    'passed': passed,
                    'time': problem_time,
                    'cost': cost,
                    'solution_length': len(solution) if solution else 0
                })

            except asyncio.TimeoutError:
                logger.error(f"[WorkflowEvaluator] {task_id}: â±ï¸ TIMEOUT")
                results.append({
                    'task_id': task_id,
                    'passed': False,
                    'time': self.timeout_per_problem,
                    'error': 'timeout'
                })

            except Exception as e:
                logger.error(f"[WorkflowEvaluator] {task_id}: âŒ ERROR: {e}")
                results.append({
                    'task_id': task_id,
                    'passed': False,
                    'time': 0,
                    'error': str(e)
                })

        total_time = time.time() - start_time

        # è®¡ç®—pass@k
        pass_at_k = num_passed / num_problems if num_problems > 0 else 0.0
        avg_time = total_time / num_problems if num_problems > 0 else 0.0

        evaluation_result = {
            'pass_at_k': pass_at_k,
            'pass_at_1': pass_at_k,  # åŒpass@k
            'num_passed': num_passed,
            'num_total': num_problems,
            'avg_time': avg_time,
            'total_time': total_time,
            'details': results
        }

        logger.info(f"[WorkflowEvaluator] ===== EVALUATION COMPLETE =====")
        logger.info(f"[WorkflowEvaluator] Pass@{num_problems}: {pass_at_k:.4f} ({num_passed}/{num_problems})")
        logger.info(f"[WorkflowEvaluator] Avg time: {avg_time:.2f}s")
        logger.info(f"[WorkflowEvaluator] Total time: {total_time:.2f}s")

        return evaluation_result

    def _test_solution(
        self,
        solution: str,
        test_code: str,
        entry_point: str
    ) -> bool:
        """
        æµ‹è¯•solutionæ˜¯å¦é€šè¿‡

        Args:
            solution: ç”Ÿæˆçš„solutionä»£ç 
            test_code: æµ‹è¯•ä»£ç 
            entry_point: å‡½æ•°å…¥å£ç‚¹

        Returns:
            æ˜¯å¦é€šè¿‡æµ‹è¯•
        """
        if not solution or not test_code:
            return False

        try:
            # åˆ›å»ºæµ‹è¯•ç¯å¢ƒ
            test_env = {}

            # æ‰§è¡Œsolutionä»£ç 
            exec(solution, test_env)

            # æ‰§è¡Œæµ‹è¯•ä»£ç 
            exec(test_code, test_env)

            # å¦‚æœæ²¡æœ‰å¼‚å¸¸ï¼Œè¯´æ˜é€šè¿‡
            return True

        except AssertionError as e:
            # æµ‹è¯•å¤±è´¥
            return False

        except Exception as e:
            # è¿è¡Œé”™è¯¯
            logger.debug(f"[WorkflowEvaluator] Solution execution error: {e}")
            return False

    def quick_test(self, workflow, num_problems: int = 1) -> float:
        """
        å¿«é€Ÿæµ‹è¯•workflowï¼ˆç”¨äºRLè®­ç»ƒï¼‰

        Args:
            workflow: Workflowå®ä¾‹
            num_problems: æµ‹è¯•é—®é¢˜æ•°ï¼ˆé»˜è®¤1ï¼Œæ›´å¿«ï¼‰

        Returns:
            pass@kåˆ†æ•°
        """
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        try:
            result = loop.run_until_complete(
                self.evaluate_workflow(workflow, num_problems=num_problems)
            )
            return result['pass_at_k']

        finally:
            loop.close()


# å•ä¾‹evaluatorï¼ˆé¿å…é‡å¤åŠ è½½ï¼‰
_global_evaluator = None


def get_evaluator(dataset: str = "HumanEval", sample_size: int = 3):
    """è·å–å…¨å±€evaluatorå•ä¾‹"""
    global _global_evaluator
    if _global_evaluator is None:
        _global_evaluator = WorkflowEvaluator(
            dataset=dataset,
            sample_size=sample_size
        )
    return _global_evaluator


if __name__ == "__main__":
    # æµ‹è¯•evaluator
    print("Testing WorkflowEvaluator...")

    evaluator = WorkflowEvaluator(sample_size=1)

    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•workflow
    class DummyWorkflow:
        async def __call__(self, problem, entry_point):
            # è¿”å›ä¸€ä¸ªç®€å•çš„solution
            solution = f"""
def {entry_point}(numbers, threshold):
    for i in range(len(numbers)):
        for j in range(i+1, len(numbers)):
            if abs(numbers[i] - numbers[j]) < threshold:
                return True
    return False
"""
            return solution, 0.01

    workflow = DummyWorkflow()

    # æµ‹è¯•
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)

    result = loop.run_until_complete(
        evaluator.evaluate_workflow(workflow, num_problems=1)
    )

    print(f"\nTest result:")
    print(f"Pass@K: {result['pass_at_k']:.4f}")
    print(f"Passed: {result['num_passed']}/{result['num_total']}")

    loop.close()
