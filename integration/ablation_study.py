"""
æ¶ˆèå®éªŒ Ablation Study
æµ‹è¯•ä¸åŒé…ç½®å¯¹å‡†ç¡®ç‡çš„å½±å“
"""

import asyncio
import sys
import os
import json
from typing import Dict, Any, List
from datetime import datetime

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'AFlow'))
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'AFlow', 'scripts'))

from scripts.logs import logger
from scripts.async_llm import create_llm_instance
import workspace.HumanEval.workflows.template.operator as operator


class AblationStudy:
    """æ¶ˆèå®éªŒç±»"""

    def __init__(self, llm_config, problems_subset):
        """
        Args:
            llm_config: LLMé…ç½®
            problems_subset: æµ‹è¯•é—®é¢˜å­é›†ï¼ˆä¾‹å¦‚å‰20ä¸ªï¼‰
        """
        self.llm_config = llm_config
        self.problems = problems_subset
        self.llm = create_llm_instance(llm_config)

    async def run_all_ablations(self):
        """è¿è¡Œæ‰€æœ‰æ¶ˆèå®éªŒ"""
        print("="*80)
        print("  æ¶ˆèå®éªŒ Ablation Study")
        print("="*80)
        print(f"æµ‹è¯•é—®é¢˜æ•°: {len(self.problems)}")
        print()

        results = {}

        # 1. Baseline: å•æ¬¡ç”Ÿæˆ (n=1)
        print("\n1ï¸âƒ£  Baseline: å•æ¬¡ç”Ÿæˆ (n=1, no ensemble)")
        results['baseline_n1'] = await self.test_config(
            n_generate=1,
            use_ensemble=False
        )

        # 2. åŒæ¬¡ç”Ÿæˆ + ensemble (n=2)
        print("\n2ï¸âƒ£  åŒæ¬¡ç”Ÿæˆ + ensemble (n=2)")
        results['ensemble_n2'] = await self.test_config(
            n_generate=2,
            use_ensemble=True
        )

        # 3. ä¸‰æ¬¡ç”Ÿæˆ + ensemble (n=3) - å½“å‰é…ç½®
        print("\n3ï¸âƒ£  ä¸‰æ¬¡ç”Ÿæˆ + ensemble (n=3) - Current")
        results['ensemble_n3'] = await self.test_config(
            n_generate=3,
            use_ensemble=True
        )

        # 4. äº”æ¬¡ç”Ÿæˆ + ensemble (n=5)
        print("\n4ï¸âƒ£  äº”æ¬¡ç”Ÿæˆ + ensemble (n=5)")
        results['ensemble_n5'] = await self.test_config(
            n_generate=5,
            use_ensemble=True
        )

        # 5. ä¸‰æ¬¡ç”Ÿæˆä½†ä¸ç”¨ensemble (n=3, take first)
        print("\n5ï¸âƒ£  ä¸‰æ¬¡ç”Ÿæˆä¸ç”¨ensemble (n=3, take first)")
        results['no_ensemble_n3'] = await self.test_config(
            n_generate=3,
            use_ensemble=False
        )

        # æ‰“å°æ€»ç»“
        self.print_summary(results)

        # ä¿å­˜ç»“æœ
        self.save_results(results)

        return results

    async def test_config(
        self,
        n_generate: int,
        use_ensemble: bool
    ) -> Dict[str, Any]:
        """
        æµ‹è¯•ç‰¹å®šé…ç½®

        Args:
            n_generate: ç”Ÿæˆå€™é€‰solutionçš„æ•°é‡
            use_ensemble: æ˜¯å¦ä½¿ç”¨ensembleé€‰æ‹©

        Returns:
            æµ‹è¯•ç»“æœ
        """
        start_time = datetime.now()

        code_gen = operator.CustomCodeGenerate(self.llm)
        sc_ensemble = operator.ScEnsemble(self.llm)

        passed = 0
        total = len(self.problems)
        details = []

        for i, (task_id, problem) in enumerate(self.problems.items(), 1):
            print(f"  [{i}/{total}] {task_id}...", end='', flush=True)

            try:
                # ç”Ÿæˆnä¸ªå€™é€‰solutions
                solutions = []
                for j in range(n_generate):
                    sol = await code_gen(
                        problem=problem['prompt'],
                        entry_point=problem['entry_point'],
                        instruction=""
                    )
                    solutions.append(sol['response'])

                # é€‰æ‹©solution
                if use_ensemble and n_generate > 1:
                    # ä½¿ç”¨ensemble
                    result = await sc_ensemble(
                        solutions=solutions,
                        problem=problem['prompt']
                    )
                    solution = result['response']
                else:
                    # å–ç¬¬ä¸€ä¸ª
                    solution = solutions[0]

                # æµ‹è¯•solution
                is_passed = self._test_solution(
                    solution,
                    problem.get('test', ''),
                    problem['entry_point']
                )

                if is_passed:
                    passed += 1
                    print(" âœ…")
                else:
                    print(" âŒ")

                details.append({
                    'task_id': task_id,
                    'passed': is_passed
                })

            except Exception as e:
                print(f" âŒ Error: {e}")
                details.append({
                    'task_id': task_id,
                    'passed': False,
                    'error': str(e)
                })

        duration = (datetime.now() - start_time).total_seconds()
        pass_rate = passed / total

        result = {
            'config': {
                'n_generate': n_generate,
                'use_ensemble': use_ensemble
            },
            'passed': passed,
            'total': total,
            'pass_rate': pass_rate,
            'duration': duration,
            'details': details
        }

        print(f"\n  ç»“æœ: {passed}/{total} = {pass_rate*100:.2f}%")
        print(f"  è€—æ—¶: {duration:.1f}ç§’")

        return result

    def _test_solution(
        self,
        solution: str,
        test_code: str,
        entry_point: str
    ) -> bool:
        """æµ‹è¯•solution"""
        if not solution or not test_code:
            return False

        try:
            test_env = {}
            exec(solution, test_env)
            exec(test_code, test_env)
            return True
        except:
            return False

    def print_summary(self, results: Dict[str, Dict]):
        """æ‰“å°æ€»ç»“"""
        print("\n" + "="*80)
        print("  æ¶ˆèå®éªŒæ€»ç»“ Ablation Study Summary")
        print("="*80)
        print()

        # æŒ‰å‡†ç¡®ç‡æ’åº
        sorted_results = sorted(
            results.items(),
            key=lambda x: x[1]['pass_rate'],
            reverse=True
        )

        print(f"{'é…ç½®':<30} {'nç”Ÿæˆ':<10} {'Ensemble':<12} {'Pass Rate':<15} {'æå‡':<10}")
        print("-"*80)

        baseline_rate = results['baseline_n1']['pass_rate']

        for name, result in sorted_results:
            config = result['config']
            n_gen = config['n_generate']
            use_ens = "âœ…" if config['use_ensemble'] else "âŒ"
            pass_rate = result['pass_rate'] * 100
            improvement = (result['pass_rate'] - baseline_rate) * 100

            # é«˜äº®å½“å‰é…ç½®
            marker = " ğŸ‘ˆ Current" if (n_gen == 3 and config['use_ensemble']) else ""

            print(f"{name:<30} {n_gen:<10} {use_ens:<12} {pass_rate:>6.2f}% ({result['passed']}/{result['total']})  {improvement:>+6.2f}%{marker}")

        print()

        # å…³é”®å‘ç°
        print("ğŸ” å…³é”®å‘ç°:")
        print(f"  â€¢ Baseline (n=1): {baseline_rate*100:.2f}%")
        print(f"  â€¢ å½“å‰é…ç½® (n=3+ensemble): {results['ensemble_n3']['pass_rate']*100:.2f}%")
        print(f"  â€¢ Ensembleçš„æå‡: {(results['ensemble_n3']['pass_rate'] - results['no_ensemble_n3']['pass_rate'])*100:.2f}%")
        print(f"  â€¢ å¢åŠ é‡‡æ ·æ¬¡æ•°(n=3â†’n=5)çš„è¾¹é™…æ”¶ç›Š: {(results['ensemble_n5']['pass_rate'] - results['ensemble_n3']['pass_rate'])*100:.2f}%")
        print()

    def save_results(self, results: Dict):
        """ä¿å­˜ç»“æœ"""
        filename = f"ablation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
        serializable = {}
        for key, val in results.items():
            serializable[key] = {
                'config': val['config'],
                'passed': val['passed'],
                'total': val['total'],
                'pass_rate': val['pass_rate'],
                'duration': val['duration']
            }

        with open(filename, 'w') as f:
            json.dump(serializable, f, indent=2)

        print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {filename}")


async def main():
    """ä¸»å‡½æ•°"""
    import yaml

    # åŠ è½½é…ç½®
    with open('deep_config_full_scale.yaml', 'r') as f:
        config = yaml.safe_load(f)

    # åŠ è½½HumanEvalé—®é¢˜
    print("åŠ è½½HumanEvalæ•°æ®é›†...")
    humaneval_path = '/root/AFlow/data/HumanEval.jsonl'
    problems = {}
    with open(humaneval_path, 'r') as f:
        for line in f:
            problem = json.loads(line)
            problems[problem['task_id']] = problem

    # é€‰æ‹©å‰20ä¸ªé—®é¢˜åšå¿«é€Ÿæ¶ˆèæµ‹è¯•
    all_ids = list(problems.keys())
    subset_ids = all_ids[:20]  # å‰20ä¸ªé—®é¢˜
    problems_subset = {pid: problems[pid] for pid in subset_ids}

    print(f"âœ… åŠ è½½ {len(problems_subset)} ä¸ªé—®é¢˜ç”¨äºæ¶ˆèå®éªŒ")
    print()

    # åˆ›å»ºæ¶ˆèå®éªŒ
    ablation = AblationStudy(
        llm_config=config['environment']['exec_llm_config'],
        problems_subset=problems_subset
    )

    # è¿è¡Œå®éªŒ
    results = await ablation.run_all_ablations()

    print("\nâœ… æ¶ˆèå®éªŒå®Œæˆ!")


if __name__ == '__main__':
    asyncio.run(main())
