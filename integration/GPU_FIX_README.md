# GPUé…ç½®ä¿®å¤æŒ‡å—
# GPU Configuration Fix Guide

**é—®é¢˜**: `CUDA available: False` - GPUé©±åŠ¨æ— æ³•è®¿é—®
**è§£å†³**: é…ç½®ç¯å¢ƒå˜é‡å¹¶éªŒè¯GPUå¯ç”¨æ€§

---

## ğŸš€ å¿«é€Ÿä¿®å¤ï¼ˆæ¨èï¼‰

### æ–¹æ³•1: ä½¿ç”¨è‡ªåŠ¨ä¿®å¤è„šæœ¬

å·²ä¸ºæ‚¨åˆ›å»ºäº†è‡ªåŠ¨ä¿®å¤è„šæœ¬ `fix_gpu_and_start.sh`ï¼Œå®ƒä¼šï¼š
- âœ“ è‡ªåŠ¨æ£€æµ‹GPUç¡¬ä»¶
- âœ“ è‡ªåŠ¨æŸ¥æ‰¾NVIDIAé©±åŠ¨åº“
- âœ“ é…ç½®ç¯å¢ƒå˜é‡
- âœ“ éªŒè¯GPUå¯ç”¨æ€§
- âœ“ å¯åŠ¨AIMEè®­ç»ƒ

**ä½¿ç”¨æ­¥éª¤:**

```bash
# 1. ä¸Šä¼ è„šæœ¬åˆ°æœåŠ¡å™¨
scp fix_gpu_and_start.sh root@YOUR_SERVER:/root/integration/

# 2. SSHåˆ°æœåŠ¡å™¨
ssh root@YOUR_SERVER

# 3. è¿›å…¥ç›®å½•å¹¶è¿è¡Œ
cd /root/integration
chmod +x fix_gpu_and_start.sh
./fix_gpu_and_start.sh
```

**è„šæœ¬ä¼šè‡ªåŠ¨:**
1. æ£€æŸ¥GPUç¡¬ä»¶
2. æŸ¥æ‰¾NVIDIAé©±åŠ¨åº“è·¯å¾„
3. è®¾ç½®LD_LIBRARY_PATH
4. éªŒè¯nvidia-smiå’ŒPyTorch CUDA
5. å¯åŠ¨è®­ç»ƒï¼ˆåå°è¿è¡Œï¼‰

**æŸ¥çœ‹è®­ç»ƒè¿›åº¦:**
```bash
tail -f /root/integration/aime_training_gpu.log
```

---

## ğŸ”§ æ‰‹åŠ¨ä¿®å¤ï¼ˆå¤‡é€‰ï¼‰

å¦‚æœè‡ªåŠ¨è„šæœ¬æ— æ³•è¿è¡Œï¼Œå¯ä»¥æ‰‹åŠ¨æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š

### æ­¥éª¤1: æ£€æŸ¥GPUç¡¬ä»¶

```bash
# æ£€æŸ¥GPUæ˜¯å¦å­˜åœ¨
lspci | grep -i nvidia
# åº”è¯¥æ˜¾ç¤º: NVIDIA GPUå‹å·ä¿¡æ¯

# æ£€æŸ¥GPUè®¾å¤‡æ–‡ä»¶
ls -l /dev/nvidia*
# åº”è¯¥æ˜¾ç¤º: nvidia0, nvidiactlç­‰è®¾å¤‡æ–‡ä»¶
```

### æ­¥éª¤2: æŸ¥æ‰¾NVIDIAé©±åŠ¨åº“

```bash
# æŸ¥æ‰¾libnvidia-ml.soåº“
find /usr /opt -name "libnvidia-ml.so*" 2>/dev/null

# å¸¸è§ä½ç½®:
# - /usr/lib64-nvidia/
# - /usr/lib/x86_64-linux-gnu/
# - /usr/local/cuda/lib64/
# - /opt/conda/lib/
```

### æ­¥éª¤3: è®¾ç½®ç¯å¢ƒå˜é‡

å‡è®¾æ‰¾åˆ°åº“åœ¨ `/usr/lib64-nvidia/`ï¼š

```bash
# æ–¹æ¡ˆA: ä¸´æ—¶è®¾ç½®ï¼ˆå½“å‰sessionï¼‰
export LD_LIBRARY_PATH=/usr/lib64-nvidia:$LD_LIBRARY_PATH
export CUDA_HOME=/usr/local/cuda
export PATH=$CUDA_HOME/bin:$PATH

# æ–¹æ¡ˆB: æ°¸ä¹…è®¾ç½®ï¼ˆæ¨èï¼‰
echo 'export LD_LIBRARY_PATH=/usr/lib64-nvidia:$LD_LIBRARY_PATH' >> ~/.bashrc
echo 'export CUDA_HOME=/usr/local/cuda' >> ~/.bashrc
echo 'export PATH=$CUDA_HOME/bin:$PATH' >> ~/.bashrc
source ~/.bashrc
```

### æ­¥éª¤4: éªŒè¯GPUå¯ç”¨

```bash
# æµ‹è¯•nvidia-smi
nvidia-smi
# åº”è¯¥æ˜¾ç¤º: GPUä¿¡æ¯ã€é©±åŠ¨ç‰ˆæœ¬ã€CUDAç‰ˆæœ¬

# æµ‹è¯•PyTorch CUDAæ”¯æŒ
python3 -c "import torch; print('CUDA available:', torch.cuda.is_available()); print('Device count:', torch.cuda.device_count())"
# åº”è¯¥æ˜¾ç¤º: CUDA available: True, Device count: 1 (æˆ–æ›´å¤š)
```

### æ­¥éª¤5: å¯åŠ¨è®­ç»ƒ

```bash
cd /root/integration

# è®¾ç½®Pythonè·¯å¾„
export PYTHONPATH=/root/AFlow:/root/integration:/root/verl-agent:$PYTHONPATH

# å¯åŠ¨è®­ç»ƒï¼ˆåå°ï¼‰
nohup python3 -u deep_train_real_workflow.py \
    --config aime_config.yaml \
    > aime_training.log 2>&1 &

# ç­‰å¾…å‡ ç§’
sleep 5

# æŸ¥çœ‹åˆå§‹æ—¥å¿—
tail -50 aime_training.log

# å®æ—¶ç›‘æ§
tail -f aime_training.log
```

---

## âš ï¸ å¸¸è§é—®é¢˜

### é—®é¢˜1: nvidia-smiä»ç„¶å¤±è´¥

**ç—‡çŠ¶:**
```
NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver.
```

**è§£å†³:**
```bash
# æ£€æŸ¥é©±åŠ¨æ˜¯å¦å®‰è£…
ls /usr/lib64-nvidia/libnvidia-ml.so*

# å¦‚æœæ–‡ä»¶å­˜åœ¨ä½†nvidia-smiå¤±è´¥ï¼Œæ£€æŸ¥è®¾å¤‡æƒé™
ls -l /dev/nvidia*

# å°è¯•é‡æ–°åŠ è½½é©±åŠ¨æ¨¡å—
sudo modprobe nvidia

# å¦‚æœä»ç„¶å¤±è´¥ï¼Œå¯èƒ½éœ€è¦é‡å¯æœåŠ¡å™¨
sudo reboot
```

### é—®é¢˜2: PyTorchä»ç„¶æ£€æµ‹ä¸åˆ°CUDA

**ç—‡çŠ¶:**
```python
>>> import torch
>>> torch.cuda.is_available()
False
```

**å¯èƒ½åŸå› :**
1. **PyTorchç‰ˆæœ¬ä¸CUDAç‰ˆæœ¬ä¸åŒ¹é…**

æ£€æŸ¥CUDAç‰ˆæœ¬:
```bash
nvcc --version  # æˆ–
nvidia-smi | grep "CUDA Version"
```

é‡æ–°å®‰è£…åŒ¹é…çš„PyTorch:
```bash
# CUDA 12.6
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126

# CUDA 11.8
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

2. **LD_LIBRARY_PATHæ²¡æœ‰ç”Ÿæ•ˆ**

åœ¨Pythonä¸­æ£€æŸ¥:
```python
import os
print("LD_LIBRARY_PATH:", os.environ.get('LD_LIBRARY_PATH'))
```

3. **éœ€è¦é‡å¯Pythonè¿›ç¨‹**

```bash
# é€€å‡ºPython
exit()

# é‡æ–°è®¾ç½®ç¯å¢ƒå˜é‡
export LD_LIBRARY_PATH=/usr/lib64-nvidia:$LD_LIBRARY_PATH

# é‡æ–°è¿›å…¥Pythonæµ‹è¯•
python3 -c "import torch; print(torch.cuda.is_available())"
```

### é—®é¢˜3: è®­ç»ƒå¯åŠ¨åç«‹å³å´©æºƒ

**æ£€æŸ¥æ—¥å¿—:**
```bash
tail -100 aime_training.log | grep -i error
```

**å¸¸è§é”™è¯¯:**
- **Out of memory**: å‡å°‘é…ç½®ä¸­çš„batch_sizeæˆ–env_num
- **Module not found**: æ£€æŸ¥PYTHONPATHè®¾ç½®
- **API error**: æ£€æŸ¥OpenAI APIå¯†é’¥

---

## ğŸ“Š æˆåŠŸå¯åŠ¨çš„æ ‡å¿—

è®­ç»ƒæˆåŠŸå¯åŠ¨åï¼Œæ‚¨åº”è¯¥åœ¨æ—¥å¿—ä¸­çœ‹åˆ°:

```
================================================================================
  REAL WORKFLOW DEEP INTEGRATION TRAINING
================================================================================

Device: cuda                          â† âœ“ ä½¿ç”¨GPU
PyTorch version: 2.8.0+cu126
CUDA available: True                  â† âœ“ CUDAå¯ç”¨

================================================================================
Loading Trainable Qwen Policy
================================================================================
[TrainableQwenPolicy] Loading Qwen model from /root/models/Qwen2.5-7B-Instruct...
[TrainableQwenPolicy] Device: cuda    â† âœ“ æ¨¡å‹åŠ è½½åˆ°GPU
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4
[TrainableQwenPolicy] Model loaded successfully

================================================================================
Starting AIME Training
================================================================================
[AIMEEvaluator] Loaded 30 AIME problems
[DeepWorkflowEnv] Dataset: AIME
Epoch 1/50, Episode 1/10              â† âœ“ è®­ç»ƒå¼€å§‹
```

**GPUä½¿ç”¨ç¡®è®¤:**
```bash
# åœ¨å¦ä¸€ä¸ªç»ˆç«¯ç›‘æ§GPU
watch -n 1 nvidia-smi

# æ‚¨åº”è¯¥çœ‹åˆ°:
# - Pythonè¿›ç¨‹ä½¿ç”¨GPU
# - æ˜¾å­˜å ç”¨ 16-20GB
# - GPUåˆ©ç”¨ç‡ > 0%
```

---

## ğŸ¯ å®Œæ•´å¯åŠ¨å‘½ä»¤æ€»ç»“

**ä¸€æ¬¡æ€§å®Œæ•´å‘½ä»¤** (å¤åˆ¶ç²˜è´´åˆ°æœåŠ¡å™¨ç»ˆç«¯):

```bash
# è¿›å…¥ç›®å½•
cd /root/integration

# è®¾ç½®æ‰€æœ‰å¿…è¦çš„ç¯å¢ƒå˜é‡
export LD_LIBRARY_PATH=/usr/lib64-nvidia:$LD_LIBRARY_PATH
export CUDA_HOME=/usr/local/cuda
export PATH=$CUDA_HOME/bin:$PATH
export PYTHONPATH=/root/AFlow:/root/integration:/root/verl-agent:$PYTHONPATH

# éªŒè¯GPU
echo "=== GPUéªŒè¯ ==="
nvidia-smi | head -20
python3 -c "import torch; print('CUDA:', torch.cuda.is_available())"

# å¯åŠ¨è®­ç»ƒ
echo ""
echo "=== å¯åŠ¨è®­ç»ƒ ==="
nohup python3 -u deep_train_real_workflow.py --config aime_config.yaml > aime_training.log 2>&1 &
echo "è®­ç»ƒå·²å¯åŠ¨ï¼Œè¿›ç¨‹ID: $!"

# ç­‰å¾…å¹¶æŸ¥çœ‹åˆå§‹æ—¥å¿—
sleep 5
tail -50 aime_training.log
```

---

## ğŸ“ ç›‘æ§å‘½ä»¤

```bash
# å®æ—¶æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
tail -f aime_training.log

# æŸ¥çœ‹Pass@Kåˆ†æ•°
grep "Pass@K" aime_training.log | tail -20

# æŸ¥çœ‹Epochè¿›åº¦
grep "Epoch" aime_training.log | tail -10

# æŸ¥çœ‹GPUä½¿ç”¨
nvidia-smi

# æŸ¥çœ‹è®­ç»ƒè¿›ç¨‹
ps aux | grep deep_train_real_workflow.py | grep -v grep

# åœæ­¢è®­ç»ƒ
kill $(ps aux | grep 'deep_train_real_workflow.py --config aime' | grep -v grep | awk '{print $2}')
```

---

## âœ… éªŒè¯æ¸…å•

è®­ç»ƒå¯åŠ¨å‰ï¼Œç¡®è®¤:
- [ ] GPUç¡¬ä»¶æ£€æµ‹: `lspci | grep -i nvidia` æœ‰è¾“å‡º
- [ ] nvidia-smiå·¥ä½œ: `nvidia-smi` æ˜¾ç¤ºGPUä¿¡æ¯
- [ ] PyTorch CUDAå¯ç”¨: `python3 -c "import torch; print(torch.cuda.is_available())"` è¿”å›True
- [ ] Qwenæ¨¡å‹å·²ä¸‹è½½: `/root/models/Qwen2.5-7B-Instruct/` ç›®å½•å­˜åœ¨ä¸”åŒ…å«4ä¸ª.safetensorsæ–‡ä»¶
- [ ] AIMEæ•°æ®é›†å·²ä¸‹è½½: `/root/AFlow/data/AIME_2024.jsonl` æ–‡ä»¶å­˜åœ¨
- [ ] é…ç½®æ–‡ä»¶æ­£ç¡®: `aime_config.yaml` ä¸­çš„APIå¯†é’¥å·²è®¾ç½®

---

**åˆ›å»ºæ—¶é—´**: 2025-10-19
**ç›®çš„**: ä¿®å¤CUDA GPUé©±åŠ¨é…ç½®é—®é¢˜å¹¶æˆåŠŸå¯åŠ¨AIMEè®­ç»ƒ
**çŠ¶æ€**: å·²å‡†å¤‡å¥½ä½¿ç”¨
