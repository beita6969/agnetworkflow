# âš ï¸ é‡è¦å‘ç°ï¼šç¼ºå°‘ Qwen æ¨¡å‹åŠ è½½
# Important Finding: Missing Qwen Model Loading

**å‘ç°æ—¶é—´ / Found**: 2025-10-09 16:45

---

## é—®é¢˜æè¿° / Problem Description

å½“å‰è®­ç»ƒ**æ­£åœ¨è¿è¡Œ**ï¼Œä½†æ˜¯**æ²¡æœ‰åŠ è½½ RL ç­–ç•¥æ¨¡å‹**ï¼

### å½“å‰çŠ¶æ€
```python
# åœ¨ deep_train.py å’Œ workers/aflow_worker.py ä¸­
self.optimizer = RLEnhancedOptimizer(
    rl_policy=None,  # âŒ æ²¡æœ‰ç­–ç•¥æ¨¡å‹ï¼
    use_rl_guidance=True,  # å¯ç”¨äº†ä½†æ— æ³•ä½¿ç”¨
    rl_weight=0.5,  # æƒé‡è®¾ç½®äº†ä½†æ²¡ç”¨ä¸Š
)
```

è¿™æ„å‘³ç€ï¼š
- âœ… AFlow çš„ MCTS ä¼˜åŒ–åœ¨å·¥ä½œ
- âŒ **æ²¡æœ‰ RL æŒ‡å¯¼** - Q-value å§‹ç»ˆä¸º 0
- âŒ **æ²¡æœ‰ç­–ç•¥è®­ç»ƒ** - GiGPO æ— æ³•è¿è¡Œ
- âŒ **æ²¡æœ‰åŠ¨ä½œå»ºè®®** - åªä½¿ç”¨çº¯ MCTS

### åº”è¯¥çš„çŠ¶æ€

æ ¹æ® `DEEP_INTEGRATION.md`ï¼Œåº”è¯¥ï¼š
```yaml
actor_rollout_ref:
  model:
    path: Qwen/Qwen2.5-7B-Instruct  # âŒ æ²¡æœ‰åŠ è½½
    use_remove_padding: true
    enable_gradient_checkpointing: true
```

---

## ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦ / Why This Matters

### æ·±åº¦é›†æˆçš„æ ¸å¿ƒä»·å€¼
1. **RL æŒ‡å¯¼ MCTS**ï¼šç­–ç•¥æ¨¡å‹çš„ Q-value ä¸ UCB åˆ†æ•°èåˆ
2. **åŒå‘å­¦ä¹ **ï¼šAFlow ç»éªŒ â†’ RL è®­ç»ƒï¼ŒRL ç­–ç•¥ â†’ AFlow æŒ‡å¯¼
3. **GiGPO è®­ç»ƒ**ï¼šé€šè¿‡å·¥ä½œæµåˆ†ç»„ä¼˜åŒ–ç­–ç•¥
4. **æ€§èƒ½æå‡**ï¼šè®ºæ–‡å£°ç§° +15-25% çš„æå‡**ä¾èµ–äº RL æŒ‡å¯¼**

### æ²¡æœ‰ RL ç­–ç•¥çš„å½±å“
```python
# optimizer_rl.py line 163
q_value = await self._get_q_value_from_policy(state)
# å½“ rl_policy=None æ—¶ï¼Œq_value = 0.0 âŒ

# optimizer_rl.py line 169
combined_score = (1 - self.rl_weight) * ucb_score + self.rl_weight * q_value
# = 0.5 * ucb_score + 0.5 * 0.0 = 0.5 * ucb_score
# ç­‰äºåªç”¨äº†ä¸€åŠçš„ MCTS åˆ†æ•°ï¼âŒ
```

---

## éœ€è¦åšä»€ä¹ˆ / What Needs to Be Done

### é€‰é¡¹ Aï¼šåŠ è½½é¢„è®­ç»ƒ Qwen æ¨¡å‹ï¼ˆæ¨èç”¨äºéªŒè¯ï¼‰

#### 1. ä¸‹è½½æ¨¡å‹
```bash
# åœ¨æœåŠ¡å™¨ä¸Š
cd /root
pip install -U huggingface_hub

# ä¸‹è½½ Qwen2.5-7B-Instruct (çº¦ 14GB)
huggingface-cli download Qwen/Qwen2.5-7B-Instruct \
    --local-dir $HOME/models/Qwen2.5-7B-Instruct \
    --local-dir-use-symlinks False
```

**æ—¶é—´**ï¼š10-20 åˆ†é’Ÿï¼ˆå–å†³äºç½‘ç»œï¼‰
**ç©ºé—´**ï¼š~14GB

#### 2. åˆ›å»ºç­–ç•¥åŒ…è£…å™¨

åˆ›å»º `/root/aflow_integration/integration/qwen_policy.py`ï¼š
```python
"""
Qwen Policy Wrapper for RL-Enhanced Optimizer
Qwen ç­–ç•¥åŒ…è£…å™¨ç”¨äº RL å¢å¼ºä¼˜åŒ–å™¨
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from typing import Any

class QwenRLPolicy:
    """
    Wrapper for Qwen model to provide RL policy interface
    åŒ…è£… Qwen æ¨¡å‹ä»¥æä¾› RL ç­–ç•¥æ¥å£
    """

    def __init__(self, model_path: str, device: str = "cuda"):
        """
        Initialize Qwen policy

        Args:
            model_path: Path to Qwen model
            device: Device to load model on
        """
        self.device = device

        print(f"Loading Qwen model from {model_path}...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,
            device_map="auto"
        )
        self.model.eval()
        print("âœ“ Qwen model loaded successfully")

    def get_q_value(self, state_repr: str) -> float:
        """
        Get Q-value estimate for a state

        Args:
            state_repr: Text representation of workflow state

        Returns:
            float: Q-value estimate
        """
        # Simple prompt-based Q-value estimation
        prompt = f"""Given this workflow state, estimate its quality score (0-1):

State: {state_repr}

Score (0-1):"""

        with torch.no_grad():
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
            outputs = self.model(**inputs, output_hidden_states=True)

            # Use last hidden state as value estimate
            # This is a simplified version - full implementation would train a value head
            hidden_state = outputs.hidden_states[-1][:, -1, :]

            # Project to scalar (simple average)
            q_value = torch.sigmoid(hidden_state.mean()).item()

        return float(q_value)

    def suggest_action(self, state_repr: str) -> str:
        """
        Suggest action/modification for workflow

        Args:
            state_repr: Text representation of workflow state

        Returns:
            str: Suggested action
        """
        prompt = f"""Suggest how to improve this workflow:

Current state: {state_repr}

Improvement suggestion:"""

        with torch.no_grad():
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=100,
                temperature=0.7,
                do_sample=True
            )
            suggestion = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Extract just the suggestion part
        if "Improvement suggestion:" in suggestion:
            suggestion = suggestion.split("Improvement suggestion:")[-1].strip()

        return suggestion
```

#### 3. ä¿®æ”¹ deep_train.py

åœ¨åˆ›å»ºç¯å¢ƒæ—¶åŠ è½½ç­–ç•¥ï¼š
```python
# deep_train.py ä¸­æ·»åŠ 
from qwen_policy import QwenRLPolicy

class DeepIntegratedTrainer:
    def __init__(self, config):
        # ... ç°æœ‰ä»£ç  ...

        # åŠ è½½ Qwen ç­–ç•¥
        if config.get('rl', {}).get('policy', {}).get('model_path'):
            model_path = config['rl']['policy']['model_path']
            print(f"Loading RL policy from {model_path}...")
            self.rl_policy = QwenRLPolicy(
                model_path=model_path,
                device=self.device
            )
            print("âœ“ RL policy loaded")
        else:
            self.rl_policy = None
            print("âš ï¸  No RL policy configured - using pure MCTS")

        # è®¾ç½®åˆ°æ‰€æœ‰ç¯å¢ƒ
        if self.rl_policy:
            self.set_rl_policy(self.rl_policy)
```

#### 4. æ›´æ–°é…ç½®æ–‡ä»¶

åœ¨ `test_config.yaml` ä¸­æ·»åŠ ï¼š
```yaml
rl:
  policy:
    model_path: "/root/models/Qwen2.5-7B-Instruct"
    model_name: "Qwen/Qwen2.5-7B-Instruct"
    temperature: 0.7
    max_tokens: 500
```

---

### é€‰é¡¹ Bï¼šå…ˆè¿è¡Œæ—  RL çš„éªŒè¯ï¼ˆå½“å‰çŠ¶æ€ï¼‰

**ä¼˜ç‚¹**ï¼š
- âœ… å¯ä»¥éªŒè¯ AFlow MCTS éƒ¨åˆ†å·¥ä½œ
- âœ… å¯ä»¥éªŒè¯æ•°æ®æµå’Œé›†æˆé€»è¾‘
- âœ… ä¸éœ€è¦ä¸‹è½½å¤§æ¨¡å‹
- âœ… è®­ç»ƒé€Ÿåº¦æ›´å¿«

**ç¼ºç‚¹**ï¼š
- âŒ ä¸æ˜¯å®Œæ•´çš„æ·±åº¦é›†æˆ
- âŒ æ— æ³•éªŒè¯ RL æŒ‡å¯¼æ•ˆæœ
- âŒ æ— æ³•è®­ç»ƒç­–ç•¥æ¨¡å‹
- âŒ æ€§èƒ½æå‡ä¼šæ‰“æŠ˜æ‰£

**å»ºè®®**ï¼š
1. **ç°åœ¨**ï¼šè®©å½“å‰è®­ç»ƒå®Œæˆï¼ŒéªŒè¯ AFlow + Claude API å·¥ä½œ
2. **ç„¶å**ï¼šä¸‹è½½ Qwen æ¨¡å‹ï¼Œæ·»åŠ ç­–ç•¥åŠ è½½ä»£ç 
3. **æœ€å**ï¼šè¿è¡Œå®Œæ•´çš„æ·±åº¦é›†æˆè®­ç»ƒ

---

### é€‰é¡¹ Cï¼šä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰

å¦‚æœ A100 40GB æ˜¾å­˜ä¸å¤Ÿï¼š
```bash
# ä¸‹è½½ Qwen2.5-1.5B (æ›´å°ï¼Œçº¦ 3GB)
huggingface-cli download Qwen/Qwen2.5-1.5B-Instruct \
    --local-dir $HOME/models/Qwen2.5-1.5B-Instruct
```

æˆ–è€…ä½¿ç”¨ Qwen2.5-3B (çº¦ 6GB)ã€‚

---

## æ—¶é—´ä¼°ç®— / Time Estimates

### å®Œæˆå½“å‰è®­ç»ƒï¼ˆæ—  RLï¼‰
- **é¢„è®¡æ—¶é—´**ï¼š5-10 åˆ†é’Ÿï¼ˆtest_configï¼‰
- **çŠ¶æ€**ï¼šæ­£åœ¨è¿è¡Œä¸­ï¼Œç­‰å¾…å®Œæˆ
- **ç›®çš„**ï¼šéªŒè¯ AFlow + Claude API + æ•°æ®æµ

### æ·»åŠ  Qwen ç­–ç•¥ï¼ˆé€‰é¡¹ Aï¼‰
- **ä¸‹è½½æ¨¡å‹**ï¼š10-20 åˆ†é’Ÿ
- **ç¼–å†™åŒ…è£…å™¨**ï¼š10-15 åˆ†é’Ÿ
- **æµ‹è¯•åŠ è½½**ï¼š5 åˆ†é’Ÿ
- **å®Œæ•´è®­ç»ƒ**ï¼š15-30 åˆ†é’Ÿï¼ˆtest_configï¼‰
- **æ€»è®¡**ï¼š~1 å°æ—¶

### ä½¿ç”¨å°æ¨¡å‹ï¼ˆé€‰é¡¹ Cï¼‰
- **ä¸‹è½½**ï¼š3-5 åˆ†é’Ÿ
- **å…¶ä»–æ­¥éª¤**ï¼šåŒä¸Š
- **æ€»è®¡**ï¼š~30 åˆ†é’Ÿ

---

## æ¨èè·¯å¾„ / Recommended Path

### ç«‹å³æ‰§è¡Œï¼ˆ5åˆ†é’Ÿï¼‰

1. **ç­‰å¾…å½“å‰è®­ç»ƒå®Œæˆ**
   - è¿™å°†éªŒè¯ AFlow + Claude API å·¥ä½œ
   - ç¡®è®¤æ•°æ®æµå’ŒåŸºç¡€è®¾æ–½æ­£ç¡®

2. **æ£€æŸ¥è®­ç»ƒç»“æœ**
   ```bash
   # åœ¨æœåŠ¡å™¨ä¸Š
   ls -la /root/aflow_integration/integration/output/test_run/
   cat /root/aflow_integration/integration/final_run.log | tail -50
   ```

### ç„¶åæ‰§è¡Œï¼ˆ30åˆ†é’Ÿï¼‰

3. **ä¸‹è½½ Qwen æ¨¡å‹**
   ```bash
   ssh root@6.tcp.ngrok.io -p 15577
   cd /root
   pip install -U huggingface_hub

   # é€‰æ‹©æ¨¡å‹å¤§å°
   # å°æ¨¡å‹ï¼ˆæ¨èæµ‹è¯•ï¼‰ï¼š
   huggingface-cli download Qwen/Qwen2.5-1.5B-Instruct \
       --local-dir /root/models/Qwen2.5-1.5B-Instruct

   # æˆ–å¤§æ¨¡å‹ï¼ˆç”Ÿäº§ï¼‰ï¼š
   huggingface-cli download Qwen/Qwen2.5-7B-Instruct \
       --local-dir /root/models/Qwen2.5-7B-Instruct
   ```

4. **åˆ›å»ºç­–ç•¥åŒ…è£…å™¨**
   - ä¸Šä¼  `qwen_policy.py` åˆ°æœåŠ¡å™¨
   - æµ‹è¯•æ¨¡å‹åŠ è½½

5. **ä¿®æ”¹é…ç½®å’Œè®­ç»ƒè„šæœ¬**
   - æ›´æ–° `test_config.yaml`
   - ä¿®æ”¹ `deep_train.py` åŠ è½½ç­–ç•¥

6. **è¿è¡Œå®Œæ•´æ·±åº¦é›†æˆè®­ç»ƒ**

---

## å½“å‰é€‰æ‹© / Current Choice

**å»ºè®®**ï¼š
1. âœ… **ç°åœ¨**ï¼šè®©å½“å‰è®­ç»ƒå®Œæˆï¼ˆé¢„è®¡5åˆ†é’Ÿï¼‰
2. â¸ï¸  **ç­‰å¾…**ï¼šè®­ç»ƒå®Œæˆåï¼Œå†³å®šæ˜¯å¦æ·»åŠ  Qwen
3. ğŸ“Š **åˆ†æ**ï¼šæŸ¥çœ‹æ—  RL çš„åŸºå‡†æ€§èƒ½
4. ğŸš€ **å¦‚æœéœ€è¦**ï¼šæ·»åŠ  Qwen å¹¶å¯¹æ¯”æ€§èƒ½

**è¯¢é—®ç”¨æˆ·**ï¼š
- æ˜¯å¦ç°åœ¨å°±éœ€è¦å®Œæ•´çš„ RL æŒ‡å¯¼ï¼Ÿ
- è¿˜æ˜¯å…ˆéªŒè¯ AFlow éƒ¨åˆ†å·¥ä½œï¼Œç„¶åå†æ·»åŠ  Qwenï¼Ÿ
- A100 Colab ä¼šè¯è¿˜èƒ½ä¿æŒå¤šä¹…ï¼Ÿ

---

## æŠ€æœ¯ç»†èŠ‚ / Technical Details

### ä¸ºä»€ä¹ˆéœ€è¦ Qwenï¼Ÿ

**è®ºæ–‡ä¸­çš„å…³é”®æœºåˆ¶**ï¼š
```
MCTS-RL Fusion:
  combined_score = (1 - Î±) * UCB(s,a) + Î± * Q_Î¸(s,a)
                   â†‘ MCTSéƒ¨åˆ†      â†‘ RLéƒ¨åˆ†ï¼ˆéœ€è¦Qwenï¼‰
```

æ²¡æœ‰ Q_Î¸(s,a)ï¼Œå°±åªæ˜¯æ™®é€šçš„ MCTSï¼

### Qwen æ¨¡å‹çš„ä½œç”¨

1. **Actor (ç­–ç•¥)**ï¼šç”Ÿæˆå·¥ä½œæµä¿®æ”¹å»ºè®®
2. **Critic (ä»·å€¼)**ï¼šè¯„ä¼°å·¥ä½œæµè´¨é‡
3. **è®­ç»ƒç›®æ ‡**ï¼šé€šè¿‡ GiGPO ä¼˜åŒ–ç­–ç•¥
4. **è¾“å…¥**ï¼šWorkflowState çš„æ–‡æœ¬è¡¨ç¤º
5. **è¾“å‡º**ï¼šQ-value æˆ– action suggestion

---

**çŠ¶æ€**ï¼šâš ï¸ **è®­ç»ƒä¸­ - ä½†ç¼ºå°‘ RL ç­–ç•¥æ¨¡å‹**
**ä¸‹ä¸€æ­¥**ï¼šç­‰å¾…å½“å‰è®­ç»ƒå®Œæˆï¼Œç„¶åå†³å®šæ˜¯å¦æ·»åŠ  Qwen

